## # 整理的一些论文

- [x] 融合上下文信息的隐式情感句判别方法

- [x] BiLSTM with Multi- Polarity Orthogonal Attention for Implicit Sentiment Analysis

- [x] Context-Specific Heterogeneous Graph Convolutional Network for Implicit Sentiment Analysis
- [x] 一种融合上下文特征的中文隐式情感分类模型
- [x] 基于混合神经网络的中文隐式情感分析

### 什么是主要的

一个简单扩展性高的模型



训练率： 5e­5、4e­5、3e­5 和 2e­5 之间

。由于 BERT 模型不稳定，为 BERT 搜索最佳随机种子和学习率

### 特征融合

```python
aspect_hidden = torch.div(torch.sum(sequence_output.masked_fill(expand_aspect_mask, 0), dim=-2),
                          torch.sum(aspect_mask.float(), dim=-1).unsqueeze(-1))
aspect_hidden = self.aspect_representation(aspect_hidden)
merged = self.dropout(torch.cat((cls_hidden, aspect_hidden), dim=-1))
sentiment = self.classifier(merged)
```

### [简单的MLM的例子](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/bert#transformers.BertForMaskedLM)

```python
from transformers import BertTokenizer, BertForMaskedLM
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForMaskedLM.from_pretrained("bert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits
```



```python
BertForMaskedLM
```



### torchinfo 查看模型参数

### BertForPreTraining相关

- **Masked Language Model（MLM）**：在句子中随机用`[MASK]`替换一部分单词，然后将句子传入 BERT 中得到每一个单词的embedding，最终用`[MASK]`的embedding预测该位置的正确单词，这一任务旨在训练模型**根据上下文理解单词的意思**；
- **Next Sentence Prediction（NSP）**：将句子对 A 和 B 输入 BERT，使用`[CLS]`的embedding进行预测 B 是否 A 的r下一句，这一任务**旨在训练模型理解预测句子间的关系。**

![img](https://gitee.com/tazimie/img/raw/master/uPic/3-3-bert-lm.png)

1. BertForPreTraining：进行MLM和NSP两个任务的预训练；
2. BertForMaskedLM：只进行 MLM 任务的预训练；
3. BertLMHeadModel：这个和上一个的区别在于，这一模型是**作为 decoder 运行的版本**；不能看全文
4. BertForNextSentencePrediction：只进行 NSP 任务的预训练。

![img](https://gitee.com/tazimie/img/raw/master/uPic/BertPreTraining%E7%9B%B8%E5%85%B3.png)


#### 四种 Fine-tune 模型

1. `one class`-BertForSequenceClassification
2. `one class`-BertForMultipleChoice
3. `class for each token`-BertForTokenClassification
4. `copy from input`-BertForQuestionAnswering
